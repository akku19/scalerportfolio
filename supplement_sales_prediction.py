# -*- coding: utf-8 -*-
"""Supplement Sales Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XmzWWJS8y25YZIGM8OiejnOvfxUM3jrv
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt

train_df = pd.read_csv('/content/drive/MyDrive/csv /TRAIN.csv')
test_df = pd.read_csv('/content/drive/MyDrive/csv /TEST_FINAL.csv')
test_df_cpy = test_df.copy()

train_df

test_df

train_df.info()

train_df.nunique()

train_df.drop(['ID','#Order'],axis=1,inplace=True)
test_df.drop(['ID'],axis=1,inplace=True)

train_df.head()



# Date
train_date_lst = []
train_year_date_lst = []
train_month_date_lst = []
train_day_date_lst = []
train_weekday_date_lst = []

for d in train_df['Date']:
  train_date_lst.append(dt.datetime.strptime(d,'%Y-%m-%d'))

for year in train_date_lst:
  train_year_date_lst.append(year.strftime('%Y'))

for month in train_date_lst:
  train_month_date_lst.append(month.strftime('%m'))

for day in train_date_lst:
  train_day_date_lst.append(day.strftime('%d'))

for weekday in train_date_lst:
  train_weekday_date_lst.append(weekday.strftime('%A'))
# print(date_lst[0].strftime('%A'))

# Date
test_date_lst = []
test_year_date_lst = []
test_month_date_lst = []
test_day_date_lst = []
test_weekday_date_lst = []

for d in test_df['Date']:
  test_date_lst.append(dt.datetime.strptime(d,'%Y-%m-%d'))

for year in test_date_lst:
  test_year_date_lst.append(year.strftime('%Y'))

for month in test_date_lst:
  test_month_date_lst.append(month.strftime('%m'))

for day in test_date_lst:
  test_day_date_lst.append(day.strftime('%d'))

for weekday in test_date_lst:
  test_weekday_date_lst.append(weekday.strftime('%A'))
# print(date_lst[0].strftime('%A'))

train_df['Year'] = pd.Series(train_year_date_lst)
train_df['Month'] = pd.Series(train_month_date_lst)
train_df['Day'] = pd.Series(train_day_date_lst)
train_df['Week Day'] = pd.Series(train_weekday_date_lst)

test_df['Year'] = pd.Series(test_year_date_lst)
test_df['Month'] = pd.Series(test_month_date_lst)
test_df['Day'] = pd.Series(test_day_date_lst)
test_df['Week Day'] = pd.Series(test_weekday_date_lst)

train_df.drop(['Date'],axis=1,inplace=True)
train_df.head()

test_df.drop(['Date'],axis=1,inplace=True)
test_df.head()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

train_df['Store_Type'] = encoder.fit_transform(train_df['Store_Type'])
train_df['Location_Type'] = encoder.fit_transform(train_df['Location_Type'])
train_df['Region_Code'] = encoder.fit_transform(train_df['Region_Code'])
train_df['Discount'] = encoder.fit_transform(train_df['Discount'])
train_df['Week Day'] = encoder.fit_transform(train_df['Week Day'])

test_df['Store_Type'] = encoder.fit_transform(test_df['Store_Type'])
test_df['Location_Type'] = encoder.fit_transform(test_df['Location_Type'])
test_df['Region_Code'] = encoder.fit_transform(test_df['Region_Code'])
test_df['Discount'] = encoder.fit_transform(test_df['Discount'])
test_df['Week Day'] = encoder.fit_transform(test_df['Week Day'])

train_df.head()

train_df.groupby(['Year','Holiday'])['Store_id'].count()

train_df['Holidays in Year'] = train_df.groupby(['Year','Holiday'])['Store_id'].transform('count')
train_df.head()

test_df['Holidays in Year'] = test_df.groupby(['Year','Holiday'])['Store_id'].transform('count')
test_df.head()

train_df.groupby(['Month','Holiday'])['Store_id'].count()

train_df['Holidays in Month'] = train_df.groupby(['Month','Holiday'])['Store_id'].transform('count')
train_df.head()

test_df['Holidays in Month'] = test_df.groupby(['Month','Holiday'])['Store_id'].transform('count')
test_df.head()

train_df.groupby(['Year','Discount'])['Store_id'].count()

train_df['Dicounts in Year'] = train_df.groupby(['Year','Discount'])['Store_id'].transform('count')
train_df.head()

test_df['Dicounts in Year'] = test_df.groupby(['Year','Discount'])['Store_id'].transform('count')
test_df.head()

train_df.groupby(['Month','Discount'])['Store_id'].count()

train_df['Dicounts in Month'] = train_df.groupby(['Month','Discount'])['Store_id'].transform('count')
train_df.head()

test_df['Dicounts in Month'] = test_df.groupby(['Month','Discount'])['Store_id'].transform('count')
test_df.head()

train_df['Year'] = train_df['Year'].astype('int')
train_df['Month'] = train_df['Month'].astype('int')
train_df['Day'] = train_df['Day'].astype('int')
train_df.info()

test_df['Year'] = test_df['Year'].astype('int')
test_df['Month'] = test_df['Month'].astype('int')
test_df['Day'] = test_df['Day'].astype('int')
test_df.info()

plt.figure(figsize=(15,10))
sns.heatmap(train_df.corr())
plt.show()

train_X = train_df.drop(['Sales'],axis=1)
train_Y = train_df['Sales']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_train_X = scaler.fit_transform(train_X)
scaled_test_X = scaler.fit_transform(test_df)

from sklearn.metrics import r2_score,mean_squared_error,accuracy_score
from sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid = train_test_split(scaled_train_X,train_Y,test_size=0.25,random_state=42)

"""# Hyper Paramter Tuning"""

from sklearn.model_selection import GridSearchCV

"""##K-NN"""

from sklearn.neighbors import KNeighborsRegressor
KNeighborsRegressor().get_params()

param_grid_knn = {'n_neighbors':[3,5,11,29],
              'metric':['euclidean','manhattan'],
              'weights':['uniform','distance']}
grid_knn = GridSearchCV(KNeighborsRegressor(), param_grid_knn, refit = True, verbose = 3,n_jobs=-1)
grid_knn.fit(X_train,Y_train)

print('\n')

Y2_train_pred = grid_knn.predict(X_train)
Y2_valid_pred = grid_knn.predict(X_valid)

print("Train Accuracy: ",r2_score(Y_train,Y2_train_pred))
print("Validation Accuracy: ",r2_score(Y_valid,Y2_valid_pred))

print('')

print("Train RMSE Error: ",np.sqrt(mean_squared_error(Y_train,Y2_train_pred)))
print("Validation RMSE Error: ",np.sqrt(mean_squared_error(Y_valid,Y2_valid_pred)))

"""## Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
DecisionTreeRegressor().get_params()

param_grid_dt = {"splitter":["best","random"],
                "max_depth" : [1,5,9,12],
                "min_samples_leaf":[1,3,5,8,10],
                "min_weight_fraction_leaf":[0.1,0.4,0.7,0.9],
                "max_features":["auto","log2","sqrt",None],
                "max_leaf_nodes":[None,10,20,40,70,90] }
grid_dt = GridSearchCV(DecisionTreeRegressor(), param_grid_dt, refit = True, verbose = 3,n_jobs=-1)
grid_dt.fit(X_train,Y_train)

print('\n')

Y3_train_pred = grid_knn.predict(X_train)
Y3_valid_pred = grid_knn.predict(X_valid)

print("Train Accuracy: ",r2_score(Y_train,Y3_train_pred))
print("Validation Accuracy: ",r2_score(Y_valid,Y3_valid_pred))

print('')

print("Train RMSE Error: ",np.sqrt(mean_squared_error(Y_train,Y3_train_pred)))
print("Validation RMSE Error: ",np.sqrt(mean_squared_error(Y_valid,Y3_valid_pred)))

"""## ExtraTree"""

from sklearn.tree import ExtraTreeRegressor
ExtraTreeRegressor().get_params()

param_grid_et = {"splitter":["best","random"],
                "max_depth" : [1,5,9,12],
                "min_samples_leaf":[1,3,5,8,10],
                "min_weight_fraction_leaf":[0.1,0.4,0.7,0.9],
                "max_features":["auto","log2","sqrt",None],
                "max_leaf_nodes":[None,10,20,40,70,90] }

grid_et = GridSearchCV(ExtraTreeRegressor(), param_grid_et, refit = True, verbose = 3, n_jobs=-1)
grid_et.fit(X_train,Y_train)

print('\n')

Y4_train_pred = grid_et.predict(X_train)
Y4_valid_pred = grid_et.predict(X_valid)

print("Train Accuracy: ",r2_score(Y_train,Y4_train_pred))
print("Validation Accuracy: ",r2_score(Y_valid,Y4_valid_pred))

print('')

print("Train RMSE Error: ",np.sqrt(mean_squared_error(Y_train,Y4_train_pred)))
print("Validation RMSE Error: ",np.sqrt(mean_squared_error(Y_valid,Y4_valid_pred)))

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor
RandomForestRegressor().get_params()

param_grid_rf = {"max_depth" : [1,5,9,12],
                "min_samples_leaf":[1,3,5,8,10],
                "min_weight_fraction_leaf":[0.1,0.4,0.7,0.9],
                "max_features":["auto","log2","sqrt"],
                "max_leaf_nodes":[10,20,40,70,90]}

grid_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, refit = True, verbose = 3, n_jobs=-1, cv=2)
grid_rf.fit(X_train,Y_train)

print('\n')

Y5_train_pred = grid_rf.predict(X_train)
Y5_valid_pred = grid_rf.predict(X_valid)

print("Train Accuracy: ",r2_score(Y_train,Y5_train_pred))
print("Validation Accuracy: ",r2_score(Y_valid,Y5_valid_pred))

print('')

print("Train RMSE Error: ",np.sqrt(mean_squared_error(Y_train,Y5_train_pred)))
print("Validation RMSE Error: ",np.sqrt(mean_squared_error(Y_valid,Y5_valid_pred)))

!pip install xgboost

"""## XGBoost"""

# !pip install xgboost

from xgboost import XGBRegressor
XGBRegressor().get_params()

param_grid_xb = {
  'max_depth':range(3,10,2),
  'min_child_weight':range(1,6,2),
  # 'gamma':[i/10.0 for i in range(0,5)],
  # 'subsample':[i/10.0 for i in range(6,10)],
  'colsample_bytree':[i/10.0 for i in range(6,10)],
  'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]
}

grid_xb = GridSearchCV(XGBRegressor(), param_grid_xb, refit = True, verbose = 3, n_jobs=-1, cv=2)
grid_xb.fit(X_train,Y_train)

print('\n')

Y6_train_pred = grid_xb.predict(X_train)
Y6_valid_pred = grid_xb.predict(X_valid)

print("Train Accuracy: ",r2_score(Y_train,Y6_train_pred))
print("Validation Accuracy: ",r2_score(Y_valid,Y6_valid_pred))

print('')

print("Train RMSE Error: ",np.sqrt(mean_squared_error(Y_train,Y6_train_pred)))
print("Validation RMSE Error: ",np.sqrt(mean_squared_error(Y_valid,Y6_valid_pred)))

grid_xb.best_params_

xgb = XGBRegressor(colsample_bytree= 0.8, max_depth= 9, min_child_weight= 5, reg_alpha= 0)
xgb.fit(scaled_train_X,train_Y)

Y_test_pred = xgb.predict(scaled_test_X)
pd.concat([test_df_cpy['ID'],pd.DataFrame(Y_test_pred,columns=['Sales'])],axis=1).to_csv('Submission.csv',index=False)
Y_test_pred

import joblib

# Save the trained model
joblib.dump(xgb, 'xgb_model.pkl')